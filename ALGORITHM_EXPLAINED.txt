╔════════════════════════════════════════════════════════════════════════════════════════╗
║                                                                                        ║
║              STRATEGY 2: KEYWORD OPTIMIZER ALGORITHM DETAILED EXPLANATION              ║
║                                                                                        ║
║                        Rule-Based Optimization with Vector Search                      ║
║                                                                                        ║
╚════════════════════════════════════════════════════════════════════════════════════════╝

================================================================================
TABLE OF CONTENTS
================================================================================

1. OVERVIEW & ARCHITECTURE
2. PHASE 1: TITLE PARSING & CONCEPT EXTRACTION
3. PHASE 2: TOKEN CLEANUP & FILTERING
4. PHASE 3: ZONE BUILDING & ORGANIZATION
5. PHASE 4: KEYWORD ENRICHMENT VIA VECTORDB
   5.1 Query Generation Strategy
   5.2 Vector Similarity Search
   5.3 Candidate Filtering & Validation
   5.4 Keyword Selection Algorithm
6. PHASE 5: FINAL TITLE ASSEMBLY
7. DATA FLOW EXAMPLE
8. CONFIGURATION & ENVIRONMENT VARIABLES


================================================================================
1. OVERVIEW & ARCHITECTURE
================================================================================

PURPOSE:
--------
This algorithm optimizes Amazon product titles by:
- Removing redundant/low-value words (quality markers, duplicates)
- Applying semantic rules to keep titles concise
- Enriching titles with high-value keywords from a vector database
- Maintaining readability while maximizing search visibility

KEY COMPONENTS:
--------------
┌─────────────────┐
│  User Input     │ → Base title + Truth data (brand, product, size, color, etc.)
└────────┬────────┘
         ↓
┌─────────────────┐
│  PARSER         │ → Breaks title into typed concept tokens (BRAND, PRODUCT, FEATURE...)
└────────┬────────┘
         ↓
┌─────────────────┐
│  OPTIMIZER      │ → Main orchestrator - applies 10 optimization steps
└────────┬────────┘
         ↓
┌─────────────────┐
│  VECTORDB       │ → SentenceTransformers-based semantic search for keyword enrichment
└────────┬────────┘
         ↓
┌─────────────────┐
│  RENDERER       │ → Assembles final title from zones A | B | C
└────────┬────────┘
         ↓
┌─────────────────┐
│  Output         │ → Optimized title (180-200 chars ideal)
└─────────────────┘

CORE PRINCIPLES:
----------------
1. SEMANTIC INTELLIGENCE: Use token types (BRAND, PRODUCT, FEATURE) not just keywords
2. ZONE-BASED ORGANIZATION: Zone A (brand+product), Zone B (features), Zone C (specs)
3. VECTOR SIMILARITY: Find related keywords using AI embeddings, not exact string matching
4. CONTEXT-AWARE FILTERING: Prevent cross-domain pollution (e.g., car keywords in home products)
5. NEW SEARCH TERM REQUIREMENT: Keywords must add truly NEW words, not rearrangements


================================================================================
2. PHASE 1: TITLE PARSING & CONCEPT EXTRACTION
================================================================================

FILE: parser.py
FUNCTION: parser.parse_title(base_title, truth)

INPUT EXAMPLE:
--------------
Title: "Mumma's LIFE 5L Dustbin for Bathroom with Lid | Plastic Inner Bucket | Steel"
Truth: {'brand': "Mumma's", 'product': 'Dustbin', 'size': 'Small', 'color': 'Black'}

PARSING PROCESS:
----------------

Step 2.1: TEXT NORMALIZATION
-----------------------------
- Remove extra spaces, normalize case
- Handle special characters (®, ™, etc.)
- Split title into segments by "|" separator

Step 2.2: BRAND EXTRACTION
---------------------------
Algorithm:
1. Check if truth['brand'] exists in title (case-insensitive)
2. Extract exact brand phrase from title
3. Mark as Token(type=BRAND, value=100, tier=TIER_0, locked=True)
   - value=100 = highest priority
   - tier=TIER_0 = most important tier
   - locked=True = cannot be removed

Example:
Input: "Mumma's LIFE 5L Dustbin..."
Truth brand: "Mumma's"
→ Token(text="Mumma's", type=BRAND, value=100)

Step 2.3: PRODUCT EXTRACTION
-----------------------------
Algorithm:
1. Look for PRODUCT patterns in title:
   - "[brand] [1-2 words] for [compatibility]" (e.g., "LIFE 5L Dustbin for Bathroom")
   - "[number][unit] [product]" (e.g., "5L Dustbin")
   - Truth product + surrounding context (e.g., "Dustbin for Bathroom with Lid")

2. Extract LONGEST matching phrase (up to 20 words)
3. Mark as Token(type=PRODUCT, value=90, tier=TIER_0, locked=True)

Special handling:
- NON_BRAND_WORDS list prevents "Motorcycle" from being detected as BRAND
- Compatibility phrases like "for Motorcycles & Scooters" kept together

Example:
Input: "LIFE 5L Dustbin for Bathroom with Lid"
Truth product: "Dustbin"
→ Token(text="LIFE 5L Dustbin for Bathroom with Lid", type=PRODUCT, value=90)

Step 2.4: COMPATIBILITY EXTRACTION
-----------------------------------
Patterns:
- "for [device]" (e.g., "for Motorcycles & Scooters")
- "compatible with [device]"
- "fits [device]"

Example:
Input: "Shock Absorber for Bajaj Pulsar NS160"
→ Token(text="for Bajaj Pulsar NS160", type=COMPATIBILITY, value=80)

Step 2.5: MATERIAL PHRASE DETECTION (SMART)
--------------------------------------------
Algorithm:
1. Check if material word (plastic, steel, aluminium, etc.) is followed by
   descriptive noun (bucket, lid, bin, pedal, handle, frame, etc.)

2. Pattern: (material) + (optional adjective) + (descriptive noun)
   Examples:
   - "Plastic Inner Bucket" → kept together as FEATURE
   - "Stainless Steel Pedal" → kept together as FEATURE
   - "Steel" (standalone) → extracted as MATERIAL

3. Why this matters:
   - Prevents "Plastic Inner Bucket" from becoming "Plastic" + "Inner Bucket"
   - Preserves semantic meaning of material+product combinations
   - Automatic detection, no hardcoded list needed

Regular expression:
pattern = rf'\b({material})\s+(?:(?:\w+\s+)?{descriptive_nouns})'

Descriptive nouns list:
bucket|bin|lid|cover|body|pedal|handle|frame|fork|tube|rod|bar|bracket|
mount|clip|ring|plate|panel|door|drawer|container|tray|basket|rack

Result:
- Multi-word phrase → Token(type=FEATURE, value=65, tier=TIER_1)
- Standalone material → Token(type=MATERIAL, value=60, tier=TIER_1)

Step 2.6: SIZE/COLOR/CAPACITY EXTRACTION
-----------------------------------------
Uses regex patterns + truth data:
- SIZE: "Small", "Large", "Mini", "XXL"
- COLOR: "Black", "Red", "Blue", "Silver"
- CAPACITY: "5L", "10 Liter", "500ml"

Step 2.7: FEATURE EXTRACTION
-----------------------------
Patterns for common features:
- "with [feature]" (e.g., "with Lid")
- "Heavy Duty", "Waterproof", "Adjustable"
- "Compact", "Portable", "Foldable"

Step 2.8: REMAINING TEXT → OTHER
---------------------------------
Any text not matched by above patterns becomes:
→ Token(type=OTHER, value=10, tier=TIER_3)
(lowest priority, will be removed if title too long)

PARSER OUTPUT:
--------------
List[Token] with each token having:
- text: The actual word/phrase
- token_type: BRAND | PRODUCT | FEATURE | MATERIAL | SIZE | COLOR | etc.
- value: Priority score (100 = highest)
- tier: TIER_0 (must keep) to TIER_3 (can remove)
- locked: True = cannot be removed by optimizer
- origin: ORIGINAL (from title) or KEYWORD (added later)


================================================================================
3. PHASE 2: TOKEN CLEANUP & FILTERING
================================================================================

FILE: optimizer.py
FUNCTIONS: _apply_implication_rules(), _remove_quality_markers(), etc.

Step 3.1: IMPLICATION RULES
----------------------------
Purpose: Remove redundant concepts when one implies another

Rules defined in token_types.py:
┌─────────────────┬──────────────────┬─────────────────────────────────┐
│ IF token has    │ AND another has  │ THEN remove                     │
├─────────────────┼──────────────────┼─────────────────────────────────┤
│ FRAGRANCE       │ SCENT            │ SCENT (fragrance is more specific)│
│ WASHABLE        │ REUSABLE         │ REUSABLE (washable implies reuse) │
│ ANTI_SLIP       │ GRIP             │ GRIP (anti-slip is more specific) │
└─────────────────┴──────────────────┴─────────────────────────────────┘

Algorithm:
for each token T1:
    for each implication rule:
        if T1 matches rule condition:
            for each token T2:
                if T2 matches rule target:
                    remove T2 from token list

Example:
Tokens: [Token("Lavender Fragrance"), Token("Pleasant Scent")]
After implication: [Token("Lavender Fragrance")]  # "Scent" removed

Step 3.2: QUALITY MARKER REMOVAL
---------------------------------
Purpose: Remove vague marketing words that don't add search value

Quality markers list:
- "Premium", "Deluxe", "Superior", "Luxury", "Best Quality"
- "High-End", "Top-Rated", "Professional Grade"

Algorithm:
for each token:
    if token.token_type == QUALITY_MARKER:
        remove token
        report removal

Why remove?
- These words are subjective and not searchable
- Amazon shoppers rarely search for "premium dustbin"
- Better to use the space for specific features

Step 3.3: IRRELEVANT 'OTHER' CHUNKS REMOVAL
--------------------------------------------
Purpose: Remove generic words that don't relate to the product

Example:
Product: Dustbin
Irrelevant chunks: "Construction", "Design", "Model"
Relevant chunks: "Bathroom", "Kitchen", "Lid"

Algorithm:
for each token with type=OTHER:
    if token.text is generic filler:
        if token.text not in truth values:
            remove token

Step 3.4: SEMANTIC GROUP ASSIGNMENT
------------------------------------
Purpose: Group similar concepts to avoid redundancy

Groups similar features (e.g., "Waterproof" + "Water-Resistant" → same group)

Step 3.5: SYNONYM CAP
---------------------
Limit: MAX_SYNONYMS = 2

Purpose: Prevent keyword stuffing with similar words

Example:
Tokens: ["Dustbin", "Trash Can", "Garbage Bin", "Waste Container"]
After cap: ["Dustbin", "Trash Can"]  # Keep only 2 best synonyms

Step 3.6: FEATURE CAP
---------------------
Limit: MAX_FEATURES = 3

Purpose: Keep title focused on most important features

Keeps features with highest value scores
Example:
Features: ["Heavy Duty" (v=35), "Compact" (v=35), "Durable" (v=35), "Portable" (v=30)]
After cap: ["Heavy Duty", "Compact", "Durable"]  # Top 3 kept

Step 3.7: DEDUPLICATION
-----------------------
Purpose: Remove exact duplicates and redundant separators

Removes:
- Exact text duplicates (case-insensitive)
- Repeated "|" separators
- Duplicate phrases like "& Lid" appearing twice

Step 3.8: ANCHOR-WORD COLLISION AVOIDANCE
------------------------------------------
Purpose: Prevent "motorcycle motorcycle" type duplicates

If "Motorcycle Shock Absorber" already contains "motorcycle", don't add it again


================================================================================
4. PHASE 3: ZONE BUILDING & ORGANIZATION
================================================================================

FILE: zone_builder.py
FUNCTION: build_zones(tokens)

ZONE STRUCTURE:
---------------
Zone A: BRAND + PRODUCT + SIZE/COLOR (Critical identity)
Zone B: MATERIALS + FEATURES (Core attributes)
Zone C: OTHER + USE_CASE + KEYWORDS (Additional context)

Final title format: Zone A | Zone B | Zone C

WHY ZONES?
----------
1. Prioritization: Most important info (brand, product) appears first
2. Readability: Logical grouping makes title scannable
3. SEO: Front-loading keywords improves search ranking
4. Character budget: Can drop Zone C items if title too long

ZONE ASSIGNMENT RULES:
----------------------
┌────────────────┬─────────────────────────────────────────────────────┐
│ Token Type     │ Assigned Zone                                       │
├────────────────┼─────────────────────────────────────────────────────┤
│ BRAND          │ Zone A (always first position)                      │
│ PRODUCT        │ Zone A (always second position)                     │
│ SIZE           │ Zone A (product identity)                           │
│ COLOR          │ Zone A (product identity)                           │
│ CAPACITY       │ Zone A (product identity)                           │
│ MATERIAL       │ Zone B (core attribute)                             │
│ FEATURE        │ Zone B (core attribute)                             │
│ SYNONYM        │ Zone C (additional search terms)                    │
│ USE_CASE       │ Zone C (additional context)                         │
│ OTHER          │ Zone C (filler content)                             │
│ KEYWORD (new)  │ Zone C (added from VectorDB)                        │
└────────────────┴─────────────────────────────────────────────────────┘

ZONE A REORDERING:
------------------
Algorithm guarantees this order within Zone A:
1. BRAND (always first)
2. CAPACITY (if present, e.g., "5L")
3. PRODUCT
4. SIZE
5. COLOR

Example:
Before: ["Mumma's", "Dustbin", "Small", "5L", "Black"]
After:  ["Mumma's", "5L", "Dustbin", "Small", "Black"]
Result: "Mumma's 5L Dustbin Small Black"


================================================================================
5. PHASE 4: KEYWORD ENRICHMENT VIA VECTORDB
================================================================================

This is the MOST IMPORTANT part for adding high-value search terms.

FILE: optimizer.py
FUNCTION: _append_keyword_phrases_if_short()

WHEN DOES IT RUN?
-----------------
ALWAYS! (No length threshold)

Previously: Only if title < 190 chars
Now: Runs on EVERY title, but won't add if already at MAX_CHARS (200)

Why always run?
- High-value keywords might justify replacing low-value words
- Vector search finds truly relevant terms based on semantic similarity
- Better to check and reject than never check at all

================================================================================
5.1 QUERY GENERATION STRATEGY
================================================================================

FILE: optimizer.py
FUNCTION: _build_keyword_db_queries()

PURPOSE:
--------
Build 3-7 concise search queries that will be used to find similar keywords
in the vector database.

BAD QUERIES (avoid):
- Single colors: "Black" → matches everything black (garbage bags, car seats, etc.)
- Generic modifiers: "Premium", "Heavy Duty" → too broad
- Single words: "Suspension" → too ambiguous

GOOD QUERIES (goal):
- Product-focused: "Shock Absorber", "Dustbin Bathroom"
- Context-aware: "Motorcycle Shock Absorber", "Kitchen Dustbin Lid"
- Multi-word: "Plastic Inner Bucket", "Stainless Steel Pedal"

QUERY BUILDING ALGORITHM:
--------------------------

Step 5.1.1: EXTRACT SIGNAL TOKENS
----------------------------------
Scan all tokens and extract text from:
- PRODUCT tokens (strongest signal)
- SYNONYM tokens (alternative product names)
- COMPATIBILITY tokens (device/use context)

Example:
Tokens: [
    Token("LIFE 5L Dustbin for Bathroom with Lid", type=PRODUCT),
    Token("Trash Can", type=SYNONYM),
    Token("for Kitchen", type=COMPATIBILITY)
]

Extracted:
- product_texts = ["LIFE 5L Dustbin for Bathroom with Lid"]
- synonym_texts = ["Trash Can"]
- compatibility_texts = ["for Kitchen"]

Step 5.1.2: DETECT CONTEXT ANCHORS
-----------------------------------
Context anchors are words that indicate the product domain:

Context groups:
┌─────────────┬──────────────────────────────────────────────────────┐
│ Group       │ Anchor Words                                         │
├─────────────┼──────────────────────────────────────────────────────┤
│ vehicle     │ motorcycle, bike, scooter, car, truck, auto          │
│ home_rooms  │ kitchen, bathroom, office, toilet, bedroom           │
│ baby        │ baby, infant, newborn                                │
│ pet         │ dog, cat, pet                                        │
└─────────────┴──────────────────────────────────────────────────────┘

Algorithm:
1. Combine title words + product token words + compatibility token words
2. Check if any anchor words appear in this combined set
3. Keep the first anchor found per group

Example:
Title: "Mumma's 5L Dustbin for Bathroom with Lid"
Words: ["mumma's", "5l", "dustbin", "bathroom", "lid"]
Detected anchors: ["bathroom"]  # from home_rooms group

Step 5.1.3: BUILD QUERY SET
----------------------------
Queries are built in priority order (most specific first):

Query Priority Order:
---------------------

Priority 1: Brand + Product
Query: "{brand} {product_phrase}"
Example: "mumma's life dustbin bathroom lid"

Priority 2: Product phrase alone
Query: "{product_phrase}"
Example: "life dustbin bathroom lid"
Max words: 6

Priority 3: Synonym phrases
Query: "{synonym_phrase}"
Example: "trash can"
Max words: 4
Limit: First 2 synonyms only

Priority 4: Anchor + Product (context-aware)
Query: "{anchor} {product_phrase}"
Example: "bathroom dustbin lid"

Query: "{product_phrase} {anchor}"
Example: "dustbin lid bathroom"

Deduplication:
- Use set() to track already-generated queries
- Preserve order (don't re-add duplicates)

Step 5.1.4: QUERY FILTERING
----------------------------
Remove bad queries:

Filter 1: Stop words / generic modifiers
-----------------------------------------
Stopwords: the, a, an, and, or, for, with, in, on, at, to, of, is, it, by
Generic modifiers: premium, heavy, duty, durable, quality, best, super
Colors: black, white, red, blue, silver, etc.

If query reduces to single word AND that word is in above lists → REJECT

Example:
Query: "Premium Black" 
→ After removing stopwords/modifiers: "Black"
→ Single word + color → REJECTED

Filter 2: Query limit
---------------------
Hard cap: MAX 7 queries (configurable via ADKRUX_DB_QUERY_LIMIT)
Default: 7
Range: 1-12

Why limit?
- Each query costs time (embedding generation + similarity search)
- Diminishing returns after ~5-7 queries
- Prevents over-matching across unrelated products

QUERY OUTPUT EXAMPLE:
---------------------
Input title: "Mumma's LIFE 5L Dustbin for Bathroom with Lid | Plastic Inner Bucket"
Truth: {brand: "Mumma's", product: "Dustbin"}

Generated queries:
1. "mumma life dustbin bathroom lid"       [brand + product]
2. "life dustbin bathroom lid"             [product phrase]
3. "bathroom life dustbin"                 [anchor + product]
4. "dustbin bathroom lid"                  [product + anchor]

Notice:
- No color-only queries
- No single-word queries
- All queries 2+ words
- All queries contain product concept


================================================================================
5.2 VECTOR SIMILARITY SEARCH
================================================================================

FILE: keyword_db.py
FUNCTION: get_top_keywords(title, limit)

VECTORDB ARCHITECTURE:
----------------------

Storage: Local .npz file (NumPy compressed)
Location: st_keywords_index/keywords_index.npz

Index contains:
- embeddings: (153,459 x 384) float32 array, L2-normalized
- keywords: (153,459,) string array
- scores: (153,459,) float32 array (importance score)
- ad_units: (153,459,) float32 array (search volume proxy)
- dataset_ids: (153,459,) string array (source dataset)

Embedding Model:
- SentenceTransformers: all-MiniLM-L6-v2
- Dimensions: 384
- Normalization: L2 (unit vectors)
- Why this model? Fast, accurate, good for short phrases

SIMILARITY SEARCH ALGORITHM:
-----------------------------

Step 5.2.1: ENCODE QUERY
-------------------------
Input query: "bathroom dustbin lid"

Process:
1. Tokenize: ["bathroom", "dustbin", "lid"]
2. Generate embedding: SentenceTransformer.encode(query)
3. L2-normalize: embedding / ||embedding||
4. Result: 384-dim unit vector

Step 5.2.2: COMPUTE COSINE SIMILARITIES
----------------------------------------
For all 153,459 keywords in index:

Formula: similarity = query_embedding · keyword_embedding

Since both vectors are L2-normalized (unit vectors):
- Dot product = Cosine similarity
- Range: -1.0 (opposite) to +1.0 (identical)
- Typical range: 0.3 to 0.95

Computation:
similarities = index_embeddings @ query_embedding  # Matrix multiplication

Result: (153,459,) array of similarity scores

Step 5.2.3: FIND TOP-N MATCHES
-------------------------------
Algorithm: NumPy argpartition + argsort

Process:
1. argpartition(-similarities, n-1)[:n]  # Get top-n indices (fast, O(n))
2. argsort(-similarities[indices])        # Sort those n indices (precise order)

Why not just argsort?
- argpartition is O(n) vs argsort O(n log n)
- For top-25 of 150k items, huge speedup
- Still get exact ordering of top results

Step 5.2.4: PER-QUERY RESULTS
------------------------------
For EACH query in the query set (e.g., 4 queries), return top-25 matches:

Example for query "bathroom dustbin lid":

Raw matches:
1. similarity=0.891, score=54.669, keyword="bathroom dustbin lid"
2. similarity=0.880, score=72.002, keyword="lid bathroom dustbin"
3. similarity=0.864, score=9.659,  keyword="bathroom dustbin with lid"
4. similarity=0.853, score=33.669, keyword="dustbin with lid bathroom"
5. similarity=0.849, score=4.712,  keyword="dustbin bathroom with lid"
...

Threshold: SIMILARITY >= 0.45
(Keywords below 0.45 similarity are rejected)

Step 5.2.5: MERGE RESULTS FROM ALL QUERIES
-------------------------------------------
Aggregate results from all 4 queries:

Query 1: "mumma life dustbin bathroom lid" → 25 matches
Query 2: "life dustbin bathroom lid" → 25 matches
Query 3: "bathroom life dustbin" → 25 matches  
Query 4: "dustbin bathroom lid" → 25 matches

Merge process:
1. Collect all candidates (max 100)
2. Deduplicate by keyword text (keep highest similarity)
3. Re-rank by combined score

Example merged results:
Total candidates before dedup: 87
Total candidates after dedup: 61


================================================================================
5.3 CANDIDATE FILTERING & VALIDATION
================================================================================

FILE: optimizer.py
FUNCTION: _append_keyword_phrases_if_short()

After getting 61 candidates from VectorDB, apply STRICT filters:

FILTER 1: SIMILARITY THRESHOLD
-------------------------------
Requirement: similarity >= 0.45

Why 0.45?
- Below 0.40: Often unrelated (e.g., "car mats" for "garbage bags")
- 0.40-0.45: Borderline (some false positives)
- Above 0.45: High confidence semantic match

Example:
✓ 0.891 similarity: "bathroom dustbin lid" for "dustbin for bathroom"
✗ 0.38 similarity: "car dustbin" for "bathroom dustbin"

FILTER 2: DUPLICATE CHECK
--------------------------
Reject if keyword text already exists in current title (case-insensitive)

Algorithm:
existing_texts = {token.text.lower() for token in all_tokens}
if keyword.lower() in existing_texts:
    REJECT

Example:
Title already has: "Bathroom Dustbin"
Keyword candidate: "bathroom dustbin lid"
→ REJECT (already contains "bathroom dustbin")

FILTER 3: NEW SEARCH TERM REQUIREMENT
--------------------------------------
This is CRITICAL for preventing redundant keywords.

Algorithm:
1. Extract meaningful words from keyword (length >= 3, not stopwords)
2. Extract meaningful words from current title
3. Normalize spelling (e.g., "mot or cycle" → "motorcycle")
4. Count NEW words in keyword that aren't in title
5. REQUIRE: At least 1 truly new word

Example 1 - GOOD:
Title: "Mumma's 5L Dustbin for Bathroom with Lid"
Title words: {mumma, dustbin, bathroom, lid}
Keyword: "bathroom flip lid dustbin"
Keyword words: {bathroom, flip, lid, dustbin}
New words: {flip}
→ ACCEPT (adds "flip")

Example 2 - BAD:
Title: "Shalimar Garbage Bags 4 Rolls Black"
Title words: {shalimar, garbage, bags, rolls, black}
Keyword: "shalimar garbage bags black"
Keyword words: {shalimar, garbage, bags, black}
New words: {}
→ REJECT (just rearranges existing words)

Spelling normalization map (handles typos in dataset):
{
    'mot or cycle': 'motorcycle',
    'mot or bike': 'motorbike',
    'abs or ber': 'absorber',
    'sus pension': 'suspension',
    'gar bage': 'garbage',
    ...
}

FILTER 4: LOW-SIGNAL MODIFIER FILTERING
----------------------------------------
Some words are technically "new" but add no search value:

Low-signal modifiers:
- shape, shaped, design, style, type, model
- years: 2020, 2021, 2022, 2023, 2024, 2025
- Pure numbers: 4, 5, 10 (context-less)

Algorithm:
new_valuable_words = {w for w in new_words if w not in low_signal_modifiers and not w.isdigit()}

Example:
Title: "Dustbin for Bathroom"
Keyword: "bathroom round shape dustbin"
New words: {round, shape}
New valuable words: {round}  # "shape" filtered out
→ ACCEPT (adds "round")

FILTER 5: CONTEXT GROUP CHECK
------------------------------
Prevents cross-domain pollution (THE KILLER FEATURE!)

Context groups (defined earlier):
- vehicle: motorcycle, bike, car, scooter, truck, etc.
- home_rooms: kitchen, bathroom, bedroom, toilet, etc.
- baby: baby, infant, newborn
- pet: dog, cat, pet

Algorithm:
1. Detect context groups in TITLE words
2. Detect context groups in KEYWORD's new words
3. If keyword introduces NEW context group → REJECT

Example 1 - REJECT:
Title: "Shalimar Garbage Bags for Kitchen"
Title context: {home_rooms}
Keyword: "car garbage bags"
Keyword new words: {car}
Keyword context: {vehicle}
Foreign context: {vehicle}
→ REJECT (adds vehicle context to home product)

Example 2 - ACCEPT:
Title: "Mumma's Dustbin for Bathroom"
Title context: {home_rooms}
Keyword: "kitchen dustbin lid"
Keyword new words: {kitchen, lid}
Keyword context: {home_rooms}
Foreign context: {}
→ ACCEPT (same context group)

Example 3 - ACCEPT:
Title: "Bajaj Pulsar Shock Absorber"
Title context: {vehicle}
Keyword: "motorcycle rear suspension"
Keyword new words: {motorcycle, rear, suspension}
Keyword context: {vehicle}
Foreign context: {}
→ ACCEPT (same context group)

FILTER 6: SPELLING VARIANT CHECK
---------------------------------
Prevent adding keywords that are just spelling variations:

Algorithm:
1. Normalize keyword: replace each word with spelling map lookup
2. Check if normalized form already in existing_texts

Example:
Title: "Motorcycle Suspension"
Keyword: "mot or cycle sus pension"
Normalized: "motorcycle suspension"
→ REJECT (spelling variant already present)

FILTER 7: PROMO WORD CHECK
---------------------------
Reject promotional/spammy terms:

Banned words: sale, offer, free, discount, cheap, #1, deal, clearance

Example:
Keyword: "dustbin sale offer"
→ REJECT (contains "sale", "offer")

FILTER 8: LENGTH CHECK (FINAL)
-------------------------------
Before accepting, simulate adding keyword to title:

Algorithm:
1. Create trial token for keyword
2. Add to token list
3. Rebuild zones
4. Render trial title
5. Check length: if > MAX_CHARS (200) → REJECT

Example:
Current title: 187 chars
Keyword: "Bathroom Flip Lid Dustbin" (25 chars + separators ≈ 28 chars)
Trial title: 187 + 28 = 215 chars
→ REJECT if > 200
→ ACCEPT if <= 200


================================================================================
5.4 KEYWORD SELECTION ALGORITHM
================================================================================

After all filters, we have a CLEAN list of candidates.

RANKING FORMULA:
----------------
For each candidate, compute combined rank score:

rank_score = (similarity_weight × similarity) + (score_weight × normalized_score)

Where:
- similarity_weight = 0.6
- score_weight = 0.4
- normalized_score = (score - min_score) / (max_score - min_score + 0.001)

Why this formula?
- Similarity (0.6 weight): Relevance to current product is most important
- Score (0.4 weight): Search volume/importance adds value
- Normalization prevents score from dominating (scores range 0-150, similarities 0-1)

SELECTION PROCESS:
------------------

Step 1: Sort candidates by rank_score (descending)
Step 2: Iterate through sorted list
Step 3: For each candidate:
    - Apply all 8 filters
    - If passes, add to title
    - If fails, continue to next
Step 4: Stop after adding 2 keywords OR running out of candidates

LIMIT: Maximum 2 keywords added per title

Why only 2?
- Prevents keyword stuffing
- Maintains readability
- Focuses on highest-value additions
- Keeps title under 200 chars

SELECTION EXAMPLE:
------------------
Sorted candidates (post-filtering):

Rank  Sim    Score  Keyword                              Status
----  -----  -----  -----------------------------------  --------
0.861 0.815  148.0  bathroom flip lid dustbin            ✅ SELECTED
0.847 0.815  130.0  bathroom smart dustbins with lid     ❌ Would exceed 200 chars
0.821 0.819  108.0  hygenic dustbin with lid bathroom    ❌ Would exceed 200 chars
...

Result: Added 1 keyword ("bathroom flip lid dustbin")

Final title length: 190 chars (within 180-200 optimal range)


================================================================================
6. PHASE 5: FINAL TITLE ASSEMBLY
================================================================================

FILE: renderer.py
FUNCTION: render_title(zones)

INPUT:
------
zones = {
    'A': [Token("Mumma's", BRAND), Token("5L", CAPACITY), Token("Dustbin", PRODUCT), Token("Small", SIZE)],
    'B': [Token("Steel", MATERIAL), Token("Plastic Inner Bucket", FEATURE), Token("Compact", FEATURE)],
    'C': [Token("Round Shape", OTHER), Token("For Bathroom", USE_CASE), Token("Black", COLOR), 
          Token("Bathroom Flip Lid Dustbin", KEYWORD)]
}

ASSEMBLY ALGORITHM:
-------------------

Step 6.1: RENDER EACH ZONE
---------------------------
For zone A, B, C:
    zone_text = " ".join([token.text.strip() for token in zone_tokens])

Result:
Zone A: "Mumma's 5L Dustbin Small"
Zone B: "Steel | Plastic Inner Bucket | Compact"
Zone C: "Round Shape | For Bathroom | Black | Bathroom Flip Lid Dustbin"

Step 6.2: JOIN ZONES WITH SEPARATOR
------------------------------------
final_title = " | ".join([zone_A, zone_B, zone_C])

Result:
"Mumma's 5L Dustbin Small | Steel | Plastic Inner Bucket | Compact | Round Shape | For Bathroom | Black | Bathroom Flip Lid Dustbin"

Step 6.3: CLEANUP
-----------------
- Remove duplicate "|" separators
- Trim extra spaces
- Normalize punctuation

Step 6.4: LENGTH CHECK
----------------------
Optimal range: 180-200 characters

Status indicators:
✅ 180-200 chars: PERFECT
⚠️  160-179 chars: SHORT (acceptable)
⚠️  201-220 chars: SLIGHTLY LONG (acceptable)
❌ <160 or >220 chars: OUT OF RANGE

TITLE CAPITALIZATION:
---------------------
Smart capitalization applied:
- Brand: Preserve original case
- Product: Title case (e.g., "Dustbin", "Shock Absorber")
- Features: Title case
- Numbers + units: Lowercase units (e.g., "5l" not "5L")


================================================================================
7. DATA FLOW EXAMPLE
================================================================================

Let's trace a complete optimization for a motorcycle product:

INPUT:
------
Base Title: "Bajaj Pulsar NS160 Rear Shock Absorber | for Motorcycles & Scooters | Heavy Duty | Premium Quality | Black"

Truth: {
    'brand': 'Bajaj',
    'product': 'Shock Absorber',
    'color': 'Black'
}

STEP-BY-STEP PROCESSING:
-------------------------

[STEP 1: PARSING]
-----------------
Parsed tokens:
1. Token("Bajaj", BRAND, value=100, locked=True)
2. Token("Pulsar NS160 Rear Shock Absorber", PRODUCT, value=90, locked=True)
3. Token("for Motorcycles & Scooters", COMPATIBILITY, value=80, locked=True)
4. Token("Heavy Duty", FEATURE, value=35)
5. Token("Premium", QUALITY_MARKER, value=5)
6. Token("Quality", QUALITY_MARKER, value=5)
7. Token("Black", COLOR, value=70, locked=True)

[STEP 2: CLEANUP]
-----------------
Removed:
- "Premium" (quality marker)
- "Quality" (quality marker)

Remaining tokens: 5

[STEP 3: ZONES]
---------------
Zone A: ["Bajaj", "Pulsar NS160 Rear Shock Absorber", "Black"]
Zone B: ["Heavy Duty"]
Zone C: ["for Motorcycles & Scooters"]

Rendered: "Bajaj Pulsar NS160 Rear Shock Absorber Black | Heavy Duty | For Motorcycles & Scooters"
Length: 92 chars (too short)

[STEP 4: KEYWORD ENRICHMENT]
-----------------------------

4.1: Build queries
------------------
Detected context: {vehicle} (from "motorcycles", "scooters")
Anchors: ["motorcycle", "scooter"]

Generated queries:
1. "bajaj pulsar shock absorber"
2. "pulsar shock absorber rear"
3. "motorcycle shock absorber rear"
4. "shock absorber motorcycle scooter"

4.2: Vector search (query 1: "bajaj pulsar shock absorber")
------------------------------------------------------------
Top matches:
1. sim=0.923, score=245.0, "bajaj pulsar suspension kit"
2. sim=0.891, score=198.0, "bajaj rear shock absorber pair"
3. sim=0.876, score=156.0, "pulsar ns160 suspension"
4. sim=0.854, score=134.0, "bajaj bike shock absorber"
5. sim=0.832, score=89.0,  "motorcycle rear suspension kit"

4.3: Vector search (query 2: "pulsar shock absorber rear")
-----------------------------------------------------------
Top matches:
1. sim=0.912, score=198.0, "bajaj rear shock absorber pair"
2. sim=0.898, score=245.0, "bajaj pulsar suspension kit"
3. sim=0.887, score=178.0, "rear shock absorber motorcycle"
4. sim=0.871, score=156.0, "pulsar ns160 suspension"
5. sim=0.845, score=134.0, "motorcycle shock absorber rear"

4.4: Merge & deduplicate
------------------------
Combined candidates: 48
After dedup: 36

4.5: Apply filters
------------------
Title words: {bajaj, pulsar, ns160, rear, shock, absorber, motorcycles, scooters, heavy, duty, black}
Title context: {vehicle}

Candidate: "bajaj pulsar suspension kit"
- ✓ Similarity 0.923 > 0.45
- ✓ Not in title
- New words: {suspension, kit}
- ✓ At least 1 new word
- ✓ No low-signal modifiers
- Keyword context: {vehicle} (from "bajaj pulsar" + "kit")
- ✓ No foreign context
- ✓ No promo words
- Trial length: 92 + 28 = 120 chars
- ✓ Under 200 chars
→ ✅ ACCEPT

Candidate: "motorcycle rear suspension kit"
- ✓ Similarity 0.832 > 0.45
- ✓ Not in title
- New words: {suspension, kit}
- ✓ At least 1 new word
- ✓ No foreign context
- ✗ "suspension" already added by previous keyword
→ ❌ REJECT (no new unique words)

4.6: Selection
--------------
Selected keywords:
1. "Bajaj Pulsar Suspension Kit" (rank=0.892)
2. "Motorcycle Rear Suspension" (rank=0.845)

[STEP 5: FINAL ASSEMBLY]
------------------------
Zone A: ["Bajaj", "Pulsar NS160 Rear Shock Absorber", "Black"]
Zone B: ["Heavy Duty"]
Zone C: ["For Motorcycles & Scooters", "Bajaj Pulsar Suspension Kit", "Motorcycle Rear Suspension"]

Final title:
"Bajaj Pulsar Ns160 Rear Shock Absorber Black | Heavy Duty | For Motorcycles & Scooters | Bajaj Pulsar Suspension Kit | Motorcycle Rear Suspension"

Length: 148 chars
Status: ⚠️ SHORT (acceptable, could add more but no more relevant keywords found)


================================================================================
8. CONFIGURATION & ENVIRONMENT VARIABLES
================================================================================

ALGORITHM CONSTANTS:
--------------------
MAX_CHARS = 200              # Maximum title length
MAX_FEATURES = 3             # Maximum feature tokens to keep
MAX_SYNONYMS = 2             # Maximum synonym tokens to keep
MAX_QUALITY_MARKERS = 0      # Quality markers allowed (always 0 = remove all)

SIMILARITY_THRESHOLD = 0.45  # Minimum cosine similarity for keyword acceptance
MAX_KEYWORDS_ADDED = 2       # Maximum keywords to add from VectorDB

ENVIRONMENT VARIABLES:
----------------------

┌──────────────────────────────┬─────────────────────────────────────────────┐
│ Variable                     │ Purpose                                     │
├──────────────────────────────┼─────────────────────────────────────────────┤
│ ADKRUX_EMBED_MODEL           │ SentenceTransformer model name              │
│                              │ Default: all-MiniLM-L6-v2                   │
├──────────────────────────────┼─────────────────────────────────────────────┤
│ ADKRUX_SHOW_DB_KEYWORDS      │ Show VectorDB search results (1=yes)        │
│                              │ Default: 1                                  │
├──────────────────────────────┼─────────────────────────────────────────────┤
│ ADKRUX_SHOW_DB_REJECTIONS    │ Show rejected keyword candidates (1=yes)    │
│                              │ Default: 1                                  │
├──────────────────────────────┼─────────────────────────────────────────────┤
│ ADKRUX_DB_TOP_PER_QUERY      │ Top-N matches per search query              │
│                              │ Default: 25                                 │
├──────────────────────────────┼─────────────────────────────────────────────┤
│ ADKRUX_DB_QUERY_LIMIT        │ Maximum number of search queries            │
│                              │ Default: 7                                  │
├──────────────────────────────┼─────────────────────────────────────────────┤
│ ADKRUX_DB_REJECTS_LIMIT      │ Max rejected keywords to display            │
│                              │ Default: 30                                 │
└──────────────────────────────┴─────────────────────────────────────────────┘

TUNING RECOMMENDATIONS:
-----------------------

For higher precision (fewer false positives):
- Increase SIMILARITY_THRESHOLD to 0.50
- Decrease DB_TOP_PER_QUERY to 15
- Decrease DB_QUERY_LIMIT to 5

For higher recall (more keyword suggestions):
- Decrease SIMILARITY_THRESHOLD to 0.40
- Increase DB_TOP_PER_QUERY to 35
- Increase DB_QUERY_LIMIT to 10
- Increase MAX_KEYWORDS_ADDED to 3

For debugging:
- Set ADKRUX_SHOW_DB_KEYWORDS=1
- Set ADKRUX_SHOW_DB_REJECTIONS=1
- Set ADKRUX_DB_REJECTS_LIMIT=50


================================================================================
SUMMARY: KEY INNOVATIONS
================================================================================

1. SMART MATERIAL PHRASE DETECTION
   - Automatically preserves "Plastic Inner Bucket" instead of splitting
   - No hardcoded patterns, uses regex + descriptive noun list
   - Prevents semantic destruction

2. CONTEXT-AWARE QUERY GENERATION
   - Builds queries from PRODUCT/SYNONYM tokens, not just truth['product']
   - Adds context anchors (motorcycle, kitchen, etc.) only when detected
   - Prevents generic single-word queries that cause cross-domain pollution

3. CONTEXT GROUP FILTERING
   - Detects product domain (vehicle, home, baby, pet)
   - Rejects keywords introducing foreign context words
   - Prevents "car garbage bags" in home products

4. NEW SEARCH TERM REQUIREMENT
   - Keywords must add truly NEW words, not rearrangements
   - Handles singular/plural normalization
   - Filters out low-signal modifiers (shape, type, year numbers)

5. ALWAYS-RUN KEYWORD ENRICHMENT
   - No length threshold check before VectorDB search
   - Every title gets keyword opportunities
   - Internal MAX_CHARS protection prevents overflow

6. HYBRID RANKING
   - Combines similarity (semantic relevance) + score (search volume)
   - Weighted 60% similarity, 40% score
   - Balances accuracy with commercial value

7. DUAL-LEVEL DEDUPLICATION
   - Text-level: Exact case-insensitive match
   - Spelling-variant: Normalized form comparison
   - Prevents "mot or cycle" when "motorcycle" exists

================================================================================
END OF ALGORITHM EXPLANATION
================================================================================

For questions or clarifications, refer to:
- parser.py: Token extraction logic
- optimizer.py: Main pipeline orchestration
- keyword_db.py: Vector similarity search
- zone_builder.py: Zone assignment rules
- renderer.py: Final title assembly
